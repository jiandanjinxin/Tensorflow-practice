{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorbot\n",
    "\n",
    "**Special thanks @MarkDaoustthat helped us with this material**\n",
    "\n",
    "In order to have a better experience follow these steps:\n",
    "\n",
    "1. Read all the notebook, try to understand what each part of the code is doing and get familiar with the implementation;\n",
    "2. For each exercise in this notebook make a copy of this notebook and try to implement what is expected. We suggest the following order for the exercises: *EXERCISE: HYPERPARAMETERS*, *EXERCISE: EXPERIMENT*, *EXERCISE: DATASET*;\n",
    "3. Troubles or doubts about the code/exercises? Ask the instructor about it or go to the end of this notebook for a possible implementation/instruction for the exercises.\n",
    "\n",
    "## Content of this notebook\n",
    "\n",
    "In this notebook you'll find a full implementation of a RNN model using the TensorFlow Estimators including comments and details about how to do it. \n",
    "\n",
    "Once you finish this notebook, you'll have a better understanding of:\n",
    "  * [TensorFlow Estimators](https://www.tensorflow.org/extend/estimators)\n",
    "  * [TensorFlow DataSets](https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/contrib/data)\n",
    "  * [RNNs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "## What is colorbot?\n",
    "\n",
    "Colorbot is a RNN model that receives a word (sequence of characters) as input and learns to predict a rgb value that better represents this word. As a result we have a color generator!\n",
    "\n",
    "![colorbot in action](../../images/colorbot_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mDT8S9C9CYtr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected version: 1.2.0 or higher\n",
      "Your TensorFlow version: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "print('Expected version: 1.2.0 or higher')\n",
    "print('Your TensorFlow version:', tf.__version__) \n",
    "\n",
    "# Feeding function for enqueue data\n",
    "from tensorflow.python.estimator.inputs.queues import feeding_functions as ff\n",
    "\n",
    "# Rnn common functions\n",
    "from tensorflow.contrib.learn.python.learn.estimators import rnn_common\n",
    "\n",
    "# Run an experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "# Model builder\n",
    "from tensorflow.python.estimator import model_fn as model_fn_lib\n",
    "\n",
    "# Plot images with pyplot\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Helpers for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UrAyWt23AtCM"
   },
   "outputs": [],
   "source": [
    "# Data files\n",
    "TRAIN_INPUT = 'data/train.csv'\n",
    "TEST_INPUT = 'data/test.csv'\n",
    "MY_TEST_INPUT = 'data/mytest.csv'\n",
    "\n",
    "# Parameters for training\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Parameters for data processing\n",
    "VOCAB_SIZE = 256\n",
    "CHARACTERS = [chr(i) for i in range(VOCAB_SIZE)]\n",
    "SEQUENCE_LENGTH_KEY = 'sequence_length'\n",
    "COLOR_NAME_KEY = 'color_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0dlZ9C27M-bS"
   },
   "outputs": [],
   "source": [
    "# Returns the column values from a CSV file as a list\n",
    "def _get_csv_column(csv_file, column_name):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        return df[column_name].tolist()\n",
    "\n",
    "# Plots a color image\n",
    "def _plot_rgb(rgb):\n",
    "    data = [[rgb]]\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function\n",
    "\n",
    "Here we are defining the input pipeline using the [Dataset API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data).\n",
    "\n",
    "One more complex transformation that we're using is called **group_by_window**, what this function does is to map each consecutive element in this dataset to a key using `key_func` and then groups the elements by key. It then applies `reduce_func` to at most `window_size` elements matching the same key. All except the final window for each key will contain `window_size` elements; the final window may be smaller.\n",
    "\n",
    "In the code below what we're doing is using the **group_by_window** transformation to batch color names that have similar length together, this makes the code more efficient since the RNN will be unrroled (approximately) the same number of steps in each batch. This avoids that we waist space and computing time :)!\n",
    "\n",
    "![](../../images/batch_by_length.png)\n",
    "*Image from [Sequence Models and the RNN API (TensorFlow Dev Summit 2017)](https://www.youtube.com/watch?v=RIR_-Xlbp7s)*\n",
    "\n",
    "** *EXERCISE DATASET (first complete the EXERCISE EXPERIMENT: change the input function bellow so it will just use normal padded_batch instead sorting the batches. Then run each model using experiments and compare the efficiency (time, global_step/sec) using TensorBoard.\n",
    "hint: to compare the implementations using tensorboard just copy the model_dir folder of both executions to the same directory (the model dir should be different at each time you run the model) and point tensorboard to it with: tensorboard --logdir=path_to_model_dirs_par)* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_fn(csv_file, batch_size, num_epochs=1, shuffle=True):\n",
    "    def _parse(line):\n",
    "        # each line: name, red, green, blue\n",
    "        # split line\n",
    "        items = tf.string_split([line],',').values\n",
    "\n",
    "        # get color (r, g, b)\n",
    "        color = tf.string_to_number(items[1:], out_type=tf.float32) / 255.0\n",
    "\n",
    "        # split color_name into a sequence of characters\n",
    "        color_name = tf.string_split([items[0]], '')\n",
    "        length = color_name.indices[-1, 1] + 1 # length = index of last char + 1\n",
    "        color_name = color_name.values\n",
    "        \n",
    "        return color, color_name, length\n",
    "\n",
    "    def _length_bin(length, cast_value=5, max_bin_id=10):\n",
    "        '''\n",
    "        Chooses a bin for a word given it's length.\n",
    "        The goal is to use group_by_window to group words\n",
    "        with the ~ same ~ length in the same bin.\n",
    "\n",
    "        Each bin will have the size of a batch, so it can train faster.\n",
    "        '''\n",
    "        bin_id = tf.cast(length / cast_value, dtype=tf.int64)\n",
    "        return tf.minimum(bin_id, max_bin_id)\n",
    "\n",
    "    def _pad_batch(ds, batch_size):\n",
    "        return ds.padded_batch(batch_size, \n",
    "                               padded_shapes=([None], [None], []),\n",
    "                               padding_values=(0.0, chr(0), tf.cast(0, tf.int64)))\n",
    "\n",
    "    def input_fn():\n",
    "        # https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data\n",
    "        dataset = (\n",
    "            tf.contrib.data.TextLineDataset(csv_file) # reading from the HD\n",
    "            .skip(1) # skip header\n",
    "            .repeat(num_epochs) # repeat dataset the number of epochs\n",
    "            .map(_parse) # parse text to variables\n",
    "            .group_by_window(key_func=lambda color, color_name, length: _length_bin(length), # choose a bin\n",
    "                             reduce_func=lambda key, ds: _pad_batch(ds, batch_size), # apply reduce funtion\n",
    "                             window_size=batch_size)\n",
    "        )\n",
    "        \n",
    "        # for our \"manual\" test we don't want to shuffle the data\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=100000)\n",
    "\n",
    "        # create iterator\n",
    "        color, color_name, length = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        features = {\n",
    "            COLOR_NAME_KEY: color_name,\n",
    "            SEQUENCE_LENGTH_KEY: length,\n",
    "        }\n",
    "\n",
    "        return features, color\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m5UJyvW5P0Sy"
   },
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(TRAIN_INPUT, BATCH_SIZE)\n",
    "test_input_fn = get_input_fn(TEST_INPUT, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'color_name': array([[b's', b'u', b'n', b'n', b'y']], dtype=object), 'sequence_length': array([5])}\n",
      "[[ 0.66274512  0.72941178  0.6156863 ]]\n"
     ]
    }
   ],
   "source": [
    "x, y = get_input_fn(TRAIN_INPUT, 1)()\n",
    "\n",
    "with tf.Session() as s:\n",
    "    print(s.run(x))\n",
    "    print(s.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Estimator model\n",
    "\n",
    "![](../../images/colorbot_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "VxXAUrYN7TvR"
   },
   "outputs": [],
   "source": [
    "def get_model_fn(rnn_cell_sizes,\n",
    "                 label_dimension,\n",
    "                 dnn_layer_sizes=[],\n",
    "                 optimizer='SGD',\n",
    "                 learning_rate=0.01):\n",
    "    \n",
    "    def model_fn(features, labels, mode):\n",
    "        \n",
    "        color_name = features[COLOR_NAME_KEY]\n",
    "        sequence_length = tf.cast(features[SEQUENCE_LENGTH_KEY], dtype=tf.int32) # int64 -> int32\n",
    "        \n",
    "        # ----------- Preparing input --------------------\n",
    "        # Creating a tf constant to hold the map char -> index\n",
    "        mapping = tf.constant(CHARACTERS, name=\"mapping\")\n",
    "        table = tf.contrib.lookup.index_table_from_tensor(mapping, dtype=tf.string)\n",
    "        int_color_name = table.lookup(color_name)\n",
    "        \n",
    "        # Converting color names to one hot representation\n",
    "        color_name_onehot = tf.one_hot(int_color_name, depth=len(CHARACTERS) + 1)\n",
    "        \n",
    "        # ---------- RNN -------------------\n",
    "        # Each RNN layer will consist of a LSTM cell\n",
    "        rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in rnn_cell_sizes]\n",
    "        \n",
    "        # Construct the layers\n",
    "        multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "        \n",
    "        # Runs the RNN model dynamically\n",
    "        # more about it at: \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                                 inputs=color_name_onehot,\n",
    "                                                 sequence_length=sequence_length,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        # Slice to keep only the last cell of the RNN\n",
    "        last_activations = rnn_common.select_last_activations(outputs,\n",
    "                                                              sequence_length)\n",
    "\n",
    "        # ------------ Dense layers -------------------\n",
    "        # Construct dense layers on top of the last cell of the RNN\n",
    "        for units in dnn_layer_sizes:\n",
    "            last_activations = tf.layers.dense(\n",
    "              last_activations, units, activation=tf.nn.relu)\n",
    "        \n",
    "        # Final dense layer for prediction\n",
    "        predictions = tf.layers.dense(last_activations, label_dimension)\n",
    "\n",
    "        # ----------- Loss and Optimizer ----------------\n",
    "        loss = None\n",
    "        train_op = None\n",
    "\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:    \n",
    "            loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "    \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:    \n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss,\n",
    "              tf.contrib.framework.get_global_step(),\n",
    "              optimizer=optimizer,\n",
    "              learning_rate=learning_rate)\n",
    "        \n",
    "        return model_fn_lib.EstimatorSpec(mode,\n",
    "                                           predictions=predictions,\n",
    "                                           loss=loss,\n",
    "                                           train_op=train_op)\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *EXERCISE HYPERPARAMETERS: try making changes to the model and see if you can improve the results.\n",
    "Run the original model, run yours and compare them using Tensorboard. What improvements do you see?  \n",
    "hint 0: change the type of RNNCell, maybe a GRUCell? Change the number of hidden layers, or add dnn layers.  \n",
    "hint 1: to compare the implementations using tensorboard just copy the model_dir folder of both executions to the same directory (the model dir should be different at each time you run the model) and point tensorboard to it with: tensorboard --logdir=path_to_model_dirs_par)* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gUHR3Mzc7Tvb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_tf_random_seed': 1, '_session_config': None, '_save_summary_steps': 100, '_model_dir': 'colorbot', '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    }
   ],
   "source": [
    "model_fn = get_model_fn(rnn_cell_sizes=[256, 128], # size of the hidden layers\n",
    "                        label_dimension=3, # since is RGB\n",
    "                        dnn_layer_sizes=[128], # size of units in the dense layers on top of the RNN\n",
    "                        optimizer='Adam', # changing optimizer to Adam\n",
    "                        learning_rate=0.01)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir='colorbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning and Evaluating\n",
    "\n",
    "** *EXERCISE EXPERIMENT: The code below works, but we can use an experiment instead. Add a cell that runs an experiment instead of interacting directly with the estimator.  \n",
    "hint 0: you'll need to change the train_input_fn definition, think about it...  \n",
    "hint 1: the change is related with the for loop* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DUZEKQrdGgZE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 0.372918\n",
      "INFO:tensorflow:Saving checkpoints for 23 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0824605.\n",
      "Evaluating epoch 0\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:00:38\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-23\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:00:39\n",
      "INFO:tensorflow:Saving dict for global step 23: global_step = 23, loss = 0.0954898\n",
      "Training epoch 1\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-23\n",
      "INFO:tensorflow:Saving checkpoints for 24 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 24, loss = 0.0994802\n",
      "INFO:tensorflow:Saving checkpoints for 46 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0647639.\n",
      "Evaluating epoch 1\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:00:41\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-46\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:00:42\n",
      "INFO:tensorflow:Saving dict for global step 46: global_step = 46, loss = 0.102245\n",
      "Training epoch 2\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-46\n",
      "INFO:tensorflow:Saving checkpoints for 47 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 47, loss = 0.0986986\n",
      "INFO:tensorflow:Saving checkpoints for 69 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0927868.\n",
      "Evaluating epoch 2\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:00:44\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-69\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:00:45\n",
      "INFO:tensorflow:Saving dict for global step 69: global_step = 69, loss = 0.0812882\n",
      "Training epoch 3\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-69\n",
      "INFO:tensorflow:Saving checkpoints for 70 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 70, loss = 0.0964924\n",
      "INFO:tensorflow:Saving checkpoints for 92 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0761905.\n",
      "Evaluating epoch 3\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:00:48\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-92\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:00:48\n",
      "INFO:tensorflow:Saving dict for global step 92: global_step = 92, loss = 0.083149\n",
      "Training epoch 4\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-92\n",
      "INFO:tensorflow:Saving checkpoints for 93 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 93, loss = 0.098203\n",
      "INFO:tensorflow:Saving checkpoints for 115 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0715288.\n",
      "Evaluating epoch 4\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:00:51\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-115\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:00:51\n",
      "INFO:tensorflow:Saving dict for global step 115: global_step = 115, loss = 0.0952814\n",
      "Training epoch 5\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-115\n",
      "INFO:tensorflow:Saving checkpoints for 116 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 116, loss = 0.10845\n",
      "INFO:tensorflow:Saving checkpoints for 138 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0390269.\n",
      "Evaluating epoch 5\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:00:54\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-138\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:00:54\n",
      "INFO:tensorflow:Saving dict for global step 138: global_step = 138, loss = 0.0750029\n",
      "Training epoch 6\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-138\n",
      "INFO:tensorflow:Saving checkpoints for 139 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 139, loss = 0.0918744\n",
      "INFO:tensorflow:Saving checkpoints for 161 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.040798.\n",
      "Evaluating epoch 6\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:00:57\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-161\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:00:58\n",
      "INFO:tensorflow:Saving dict for global step 161: global_step = 161, loss = 0.0731551\n",
      "Training epoch 7\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-161\n",
      "INFO:tensorflow:Saving checkpoints for 162 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 162, loss = 0.0924742\n",
      "INFO:tensorflow:Saving checkpoints for 184 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0467871.\n",
      "Evaluating epoch 7\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:00\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-184\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:01\n",
      "INFO:tensorflow:Saving dict for global step 184: global_step = 184, loss = 0.0960772\n",
      "Training epoch 8\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-184\n",
      "INFO:tensorflow:Saving checkpoints for 185 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 185, loss = 0.11175\n",
      "INFO:tensorflow:Saving checkpoints for 207 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0249051.\n",
      "Evaluating epoch 8\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:03\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-207\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:04\n",
      "INFO:tensorflow:Saving dict for global step 207: global_step = 207, loss = 0.0785817\n",
      "Training epoch 9\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-207\n",
      "INFO:tensorflow:Saving checkpoints for 208 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 208, loss = 0.0906201\n",
      "INFO:tensorflow:Saving checkpoints for 230 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.018581.\n",
      "Evaluating epoch 9\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:06\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-230\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:07\n",
      "INFO:tensorflow:Saving dict for global step 230: global_step = 230, loss = 0.0698065\n",
      "Training epoch 10\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-230\n",
      "INFO:tensorflow:Saving checkpoints for 231 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 231, loss = 0.0878039\n",
      "INFO:tensorflow:Saving checkpoints for 253 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0322558.\n",
      "Evaluating epoch 10\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:09\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-253\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:10\n",
      "INFO:tensorflow:Saving dict for global step 253: global_step = 253, loss = 0.084126\n",
      "Training epoch 11\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-253\n",
      "INFO:tensorflow:Saving checkpoints for 254 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 254, loss = 0.0965165\n",
      "INFO:tensorflow:Saving checkpoints for 276 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0111067.\n",
      "Evaluating epoch 11\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:12\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-276\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:13\n",
      "INFO:tensorflow:Saving dict for global step 276: global_step = 276, loss = 0.0731877\n",
      "Training epoch 12\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-276\n",
      "INFO:tensorflow:Saving checkpoints for 277 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 277, loss = 0.0915138\n",
      "INFO:tensorflow:Saving checkpoints for 299 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00214087.\n",
      "Evaluating epoch 12\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:15\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-299\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:16\n",
      "INFO:tensorflow:Saving dict for global step 299: global_step = 299, loss = 0.0762868\n",
      "Training epoch 13\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-299\n",
      "INFO:tensorflow:Saving checkpoints for 300 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 300, loss = 0.0893422\n",
      "INFO:tensorflow:Saving checkpoints for 322 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00229317.\n",
      "Evaluating epoch 13\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:18\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-322\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:18\n",
      "INFO:tensorflow:Saving dict for global step 322: global_step = 322, loss = 0.0764978\n",
      "Training epoch 14\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-322\n",
      "INFO:tensorflow:Saving checkpoints for 323 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 323, loss = 0.0883323\n",
      "INFO:tensorflow:Saving checkpoints for 345 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0128561.\n",
      "Evaluating epoch 14\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:21\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-345\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:22\n",
      "INFO:tensorflow:Saving dict for global step 345: global_step = 345, loss = 0.0888286\n",
      "Training epoch 15\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-345\n",
      "INFO:tensorflow:Saving checkpoints for 346 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 346, loss = 0.0935682\n",
      "INFO:tensorflow:Saving checkpoints for 368 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0101696.\n",
      "Evaluating epoch 15\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:24\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-368\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:25\n",
      "INFO:tensorflow:Saving dict for global step 368: global_step = 368, loss = 0.0763375\n",
      "Training epoch 16\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-368\n",
      "INFO:tensorflow:Saving checkpoints for 369 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 369, loss = 0.0789261\n",
      "INFO:tensorflow:Saving checkpoints for 391 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.016138.\n",
      "Evaluating epoch 16\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:27\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-391\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:28\n",
      "INFO:tensorflow:Saving dict for global step 391: global_step = 391, loss = 0.0655067\n",
      "Training epoch 17\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-391\n",
      "INFO:tensorflow:Saving checkpoints for 392 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 392, loss = 0.0728631\n",
      "INFO:tensorflow:Saving checkpoints for 414 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0508708.\n",
      "Evaluating epoch 17\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:30\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-414\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:31\n",
      "INFO:tensorflow:Saving dict for global step 414: global_step = 414, loss = 0.0908748\n",
      "Training epoch 18\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-414\n",
      "INFO:tensorflow:Saving checkpoints for 415 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 415, loss = 0.0916891\n",
      "INFO:tensorflow:Saving checkpoints for 437 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0252501.\n",
      "Evaluating epoch 18\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:34\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-437\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:34\n",
      "INFO:tensorflow:Saving dict for global step 437: global_step = 437, loss = 0.0879458\n",
      "Training epoch 19\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-437\n",
      "INFO:tensorflow:Saving checkpoints for 438 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 438, loss = 0.0842887\n",
      "INFO:tensorflow:Saving checkpoints for 460 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0136866.\n",
      "Evaluating epoch 19\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:37\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-460\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:37\n",
      "INFO:tensorflow:Saving dict for global step 460: global_step = 460, loss = 0.0658603\n",
      "Training epoch 20\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-460\n",
      "INFO:tensorflow:Saving checkpoints for 461 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 461, loss = 0.071634\n",
      "INFO:tensorflow:Saving checkpoints for 483 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0096562.\n",
      "Evaluating epoch 20\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:40\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-483\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:40\n",
      "INFO:tensorflow:Saving dict for global step 483: global_step = 483, loss = 0.073327\n",
      "Training epoch 21\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-483\n",
      "INFO:tensorflow:Saving checkpoints for 484 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 484, loss = 0.0735931\n",
      "INFO:tensorflow:Saving checkpoints for 506 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0105215.\n",
      "Evaluating epoch 21\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:43\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-506\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:43\n",
      "INFO:tensorflow:Saving dict for global step 506: global_step = 506, loss = 0.0606367\n",
      "Training epoch 22\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-506\n",
      "INFO:tensorflow:Saving checkpoints for 507 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 507, loss = 0.0670767\n",
      "INFO:tensorflow:Saving checkpoints for 529 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00940824.\n",
      "Evaluating epoch 22\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:46\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-529\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:46\n",
      "INFO:tensorflow:Saving dict for global step 529: global_step = 529, loss = 0.0595843\n",
      "Training epoch 23\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-529\n",
      "INFO:tensorflow:Saving checkpoints for 530 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 530, loss = 0.0613532\n",
      "INFO:tensorflow:Saving checkpoints for 552 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00288535.\n",
      "Evaluating epoch 23\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:49\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-552\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:49\n",
      "INFO:tensorflow:Saving dict for global step 552: global_step = 552, loss = 0.0563596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 24\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-552\n",
      "INFO:tensorflow:Saving checkpoints for 553 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 553, loss = 0.052846\n",
      "INFO:tensorflow:Saving checkpoints for 575 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00718242.\n",
      "Evaluating epoch 24\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:52\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-575\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:52\n",
      "INFO:tensorflow:Saving dict for global step 575: global_step = 575, loss = 0.0558685\n",
      "Training epoch 25\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-575\n",
      "INFO:tensorflow:Saving checkpoints for 576 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 576, loss = 0.0507478\n",
      "INFO:tensorflow:Saving checkpoints for 598 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00942253.\n",
      "Evaluating epoch 25\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:55\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-598\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:55\n",
      "INFO:tensorflow:Saving dict for global step 598: global_step = 598, loss = 0.0541042\n",
      "Training epoch 26\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-598\n",
      "INFO:tensorflow:Saving checkpoints for 599 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 599, loss = 0.0585796\n",
      "INFO:tensorflow:Saving checkpoints for 621 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00962922.\n",
      "Evaluating epoch 26\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:01:58\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-621\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:01:58\n",
      "INFO:tensorflow:Saving dict for global step 621: global_step = 621, loss = 0.0639904\n",
      "Training epoch 27\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-621\n",
      "INFO:tensorflow:Saving checkpoints for 622 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 622, loss = 0.0577728\n",
      "INFO:tensorflow:Saving checkpoints for 644 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0333434.\n",
      "Evaluating epoch 27\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:01\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-644\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:01\n",
      "INFO:tensorflow:Saving dict for global step 644: global_step = 644, loss = 0.0704458\n",
      "Training epoch 28\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-644\n",
      "INFO:tensorflow:Saving checkpoints for 645 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 645, loss = 0.0570882\n",
      "INFO:tensorflow:Saving checkpoints for 667 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.003277.\n",
      "Evaluating epoch 28\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:04\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-667\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:04\n",
      "INFO:tensorflow:Saving dict for global step 667: global_step = 667, loss = 0.071586\n",
      "Training epoch 29\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-667\n",
      "INFO:tensorflow:Saving checkpoints for 668 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 668, loss = 0.048364\n",
      "INFO:tensorflow:Saving checkpoints for 690 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0145544.\n",
      "Evaluating epoch 29\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:07\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-690\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:07\n",
      "INFO:tensorflow:Saving dict for global step 690: global_step = 690, loss = 0.0513626\n",
      "Training epoch 30\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-690\n",
      "INFO:tensorflow:Saving checkpoints for 691 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 691, loss = 0.0399841\n",
      "INFO:tensorflow:Saving checkpoints for 713 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0139253.\n",
      "Evaluating epoch 30\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:10\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-713\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:10\n",
      "INFO:tensorflow:Saving dict for global step 713: global_step = 713, loss = 0.0565116\n",
      "Training epoch 31\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-713\n",
      "INFO:tensorflow:Saving checkpoints for 714 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 714, loss = 0.0398536\n",
      "INFO:tensorflow:Saving checkpoints for 736 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0164084.\n",
      "Evaluating epoch 31\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:13\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-736\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:13\n",
      "INFO:tensorflow:Saving dict for global step 736: global_step = 736, loss = 0.0520479\n",
      "Training epoch 32\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-736\n",
      "INFO:tensorflow:Saving checkpoints for 737 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 737, loss = 0.0353314\n",
      "INFO:tensorflow:Saving checkpoints for 759 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0103122.\n",
      "Evaluating epoch 32\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:16\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-759\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:16\n",
      "INFO:tensorflow:Saving dict for global step 759: global_step = 759, loss = 0.0531136\n",
      "Training epoch 33\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-759\n",
      "INFO:tensorflow:Saving checkpoints for 760 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 760, loss = 0.0276007\n",
      "INFO:tensorflow:Saving checkpoints for 782 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00283034.\n",
      "Evaluating epoch 33\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:19\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-782\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:19\n",
      "INFO:tensorflow:Saving dict for global step 782: global_step = 782, loss = 0.0481051\n",
      "Training epoch 34\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-782\n",
      "INFO:tensorflow:Saving checkpoints for 783 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 783, loss = 0.0264262\n",
      "INFO:tensorflow:Saving checkpoints for 805 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000906388.\n",
      "Evaluating epoch 34\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:22\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-805\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:22\n",
      "INFO:tensorflow:Saving dict for global step 805: global_step = 805, loss = 0.0490656\n",
      "Training epoch 35\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-805\n",
      "INFO:tensorflow:Saving checkpoints for 806 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 806, loss = 0.0225186\n",
      "INFO:tensorflow:Saving checkpoints for 828 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00639401.\n",
      "Evaluating epoch 35\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:25\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-828\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 828: global_step = 828, loss = 0.0510266\n",
      "Training epoch 36\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-828\n",
      "INFO:tensorflow:Saving checkpoints for 829 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 829, loss = 0.0241904\n",
      "INFO:tensorflow:Saving checkpoints for 851 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0193809.\n",
      "Evaluating epoch 36\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:28\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-851\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:29\n",
      "INFO:tensorflow:Saving dict for global step 851: global_step = 851, loss = 0.0489838\n",
      "Training epoch 37\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-851\n",
      "INFO:tensorflow:Saving checkpoints for 852 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 852, loss = 0.0159903\n",
      "INFO:tensorflow:Saving checkpoints for 874 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00673583.\n",
      "Evaluating epoch 37\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:31\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-874\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:31\n",
      "INFO:tensorflow:Saving dict for global step 874: global_step = 874, loss = 0.0453029\n",
      "Training epoch 38\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-874\n",
      "INFO:tensorflow:Saving checkpoints for 875 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 875, loss = 0.0148224\n",
      "INFO:tensorflow:Saving checkpoints for 897 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0252576.\n",
      "Evaluating epoch 38\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:34\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-897\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:34\n",
      "INFO:tensorflow:Saving dict for global step 897: global_step = 897, loss = 0.048416\n",
      "Training epoch 39\n",
      "--------------------\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-897\n",
      "INFO:tensorflow:Saving checkpoints for 898 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:step = 898, loss = 0.0233025\n",
      "INFO:tensorflow:Saving checkpoints for 920 into colorbot/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00560891.\n",
      "Evaluating epoch 39\n",
      "--------------------\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-15:02:37\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-920\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-15:02:37\n",
      "INFO:tensorflow:Saving dict for global step 920: global_step = 920, loss = 0.0482685\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 40\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print('Training epoch %d' % i)\n",
    "    print('-' * 20)\n",
    "    estimator.train(input_fn=train_input_fn)\n",
    "    print('Evaluating epoch %d' % i)\n",
    "    print('-' * 20)\n",
    "    estimator.evaluate(input_fn = test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(estimator, input_file):\n",
    "    preds = estimator.predict(input_fn=get_input_fn(input_file, 1, shuffle=False))\n",
    "    color_names = _get_csv_column(input_file, 'name')\n",
    "\n",
    "    print()\n",
    "    for p, name in zip(preds, color_names):\n",
    "        color = tuple(map(int, p * 255))\n",
    "        print(name + ',', 'rgb:', color)\n",
    "        _plot_rgb(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from colorbot/model.ckpt-920\n",
      "orange, rgb: (218, 101, 49)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmwa6GNCCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a6b1b160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow orange, rgb: (216, 107, 51)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7JsKjCAiSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a6ab6780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adfgasdgasd, rgb: (181, 182, 141)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAB7tJREFUeJzt3W2IXGcZxvH/lYSk0CZrNummi4bGYFCiSKtL8QVfMBFq\nhVhotRXFBFIiBEG0fggE/NB+sLX4SgUNURpb0NiANNIUbZIWvzTRBV9CKuluiuLGtNoo0Vpaqd5+\nmLNhss7szO6ZOXN75vrBMuflmXmew16cmbNn73kUEZgN2pJBD8AMHERLwkG0FBxES8FBtBQcREvB\nQbQUHERLwUG0FJYNegDtjIxcGWNjo4Mehi3A9PTMCxFx9WKemzaIY2OjfOObnxv0MGwBPnzTnX9Y\n7HP91mwpOIiWgoNoKTiIloKDaCk4iJZCqSBKGpX0uKSp4nH1PG1XSZqRdH+ZPq2eyp4R9wDHImIT\ncKxYb+du4Ocl+7OaKhvEjwAHiuUDwM2tGkl6O7AO+FnJ/qymygZxXUScL5afoxG2y0haAnwF+EKn\nF5O0S9KkpMmLf/9nyaHZ/5OOt/gkHQWuabFrb/NKRISkViWBu4EjETEjad6+ImIfsA9g06b1Li8c\nIh2DGBFb2+2T9Lyk8Yg4L2kc+HOLZu8E3iNpN3AVsFzSixEx3+dJGzJl/+nhMLAduKd4fGRug4j4\nxOyypB3AhENoc5X9jHgP8EFJU8DWYh1JE5L2lx2cDY9SZ8SIuABsabF9ErijxfYHgAfK9Gn15Dsr\nloKDaCk4iJaCg2gpOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWgoNoKfS9eErSdZKeknRa0m8l3Vam\nT6unKoqnXgI+FRFvBm4Evi7pNSX7tZrpe/FURDwTEVPF8p9o/Bf3or66zOqr78VTzSTdACwHzpbs\n12qmiuKp2dcZBx4EtkfEf9q02QXsArh6rG2tvtVQFcVTSFoFPArsjYgT8/TlKr4hVfatebZ4CtoU\nT0laDvwY+H5EHCrZn9VUFcVTHwPeC+yQ9Ovi57qS/VrN9L14KiIeAh4q04/Vn++sWAoOoqXgIFoK\nDqKl4CBaCg6ipeAgWgoOoqXgIFoKDqKl4CBaCg6ipeAgWgo9CaKkGyWdkTQt6X8KqCStkHSw2H9S\n0oZe9Gv1UTqIkpYC3wI+BGwGPi5p85xmO4G/RcQbgK8B95bt1+qlF2fEG4DpiHg2Iv4F/JBGdV+z\n5mq/Q8AWdZr9x4ZKL4L4WuCPTeszxbaWbSLiVeAisGbuC3kKtOGV6mIlIvZFxERETIysunLQw7EK\n9SKI54D1TeuvK7a1bCNpGTACXOhB31YTvQjiL4FNkl5fVOzdTqO6r1lztd+twPGIcLmoXVJ2Lj4i\n4lVJnwF+CiwFvhcRpyXdBUxGxGHgu8CDkqaBv9IIq9klpYMIEBFHgCNztn2xafll4KO96MvqKdXF\nig0vB9FScBAtBQfRUnAQLQUH0VJwEC0FB9FScBAtBQfRUnAQLQUH0VJwEC2Fqqr4Pi/p6WIuvmOS\nru1Fv1YfVVXx/QqYiIi30iie+nLZfq1eKqnii4gnIuKlYvUEjXICs0uqquJrthN4rNUOV/ENr0ov\nViR9EpgA7mu131V8w6sXpQLdVPEhaSuNiSTfFxGv9KBfq5FKqvgkXQ98B9gWES0njrThVjqIxTc3\nzFbx/Q740WwVn6RtRbP7gKuAh4u5+OaWm9qQq6qKr+1Uu2bgOyuWhINoKTiIloKDaCk4iJaCg2gp\nOIiWgoNoKTiIloKDaCk4iJaCg2gpOIiWQiVVfE3tbpEUkiZ60a/VR1VVfEhaCXwWOFm2T6ufqubi\nA7ibxmSQL/egT6uZSqr4JL0NWB8Rj873Qq7iG159v1iRtAT4KnBnp7au4hteVczFtxJ4C/CkpN8D\n7wAO+4LFmvW9ii8iLkbE2ojYEBEbaHzTw7aImOxB31YTVVXxmc2rkiq+Odvf34s+rV58Z8VScBAt\nBQfRUnAQLQUH0VJQRAx6DC1J+gdwZtDj6JO1wAuDHkQfvDEiVi7miT35802fnImIWt59kTRZx2OT\ntOibFH5rthQcREshcxD3DXoAfVTXY1v0caW9WLHhkvmMaEMkTRAljUp6XNJU8bi6Tbt/F9/Dnf67\nuLuYGm6FpIPF/pOSNlQ/yoXr4rh2SPpL0+/pjo4vGhEpfmhMi7anWN4D3Num3YuDHmuXx7MUOAts\nBJYDvwE2z2mzG/h2sXw7cHDQ4+7Rce0A7l/I66Y5I9IouDpQLB8Abh7gWHqhm6Ky5mM+BGyRpArH\nuBjdFsstSKYgrouI88Xyc8C6Nu2uKAqsTkjKHNZupoa71CYa/2B8EVhTyegWr9sp724pZqM9JGl9\ni/2XqfTOiqSjwDUtdu1tXomIkNTucv7aiDgnaSNwXNKpiDjb67FaKT8BfhARr0j6NI2z/gfme0Kl\nQYx55luR9Lyk8Yg4L2kcaDlDVUScKx6flfQkcD2NzyzZdDM13GybGUnLgBHgQjXDW7SOxxURzcew\nny6mRc701nwY2F4sbwcemdtA0mpJK4rltcC7gacrG+HCdJwajsuP+VbgeBSf9hPrZsq78abVbTRq\nmeY36KuwpiutNcAxYAo4CowW2yeA/cXyu4BTNK7UTgE7Bz3uDsd0E/AMjTP23mLbXTSqGAGuAB4G\npoFfABsHPeYeHdeXgNPF7+kJ4E2dXtN3ViyFTG/NNsQcREvBQbQUHERLwUG0FBxES8FBtBQcREvh\nvzRXoB0U0FaHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a62c8828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple blue, rgb: (106, 96, 217)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/BdMopseQkgliAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a62e74e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple red, rgb: (230, 53, 102)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkymbD8QwVMYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a6202048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple, rgb: (101, 23, 155)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESGa2mmVAIcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a61d6f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water, rgb: (207, 197, 201)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+aBB+LS4HAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a61ae080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pink, rgb: (271, 151, 192)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAsM4m4cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a6111b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock, rgb: (95, 54, 38)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n66qas7oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a60e9c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(estimator, MY_TEST_INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model predictions\n",
    "\n",
    "In order to load the pre-trained model we can just create an estimator using the model_fn and use the model_dir that contains the pre-trained model files in this case it's 'pretrained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_tf_random_seed': 1, '_session_config': None, '_save_summary_steps': 100, '_model_dir': 'pretrained', '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000}\n",
      "\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from pretrained/model.ckpt-8069\n",
      "orange, rgb: (233, 124, 19)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a4da5ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow orange, rgb: (232, 149, 88)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTECbOXkzj6IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a4d29da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adfgasdgasd, rgb: (201, 184, 157)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n26As+unt9gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a5f83b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple blue, rgb: (51, 13, 220)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a60aa3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple red, rgb: (152, 40, 61)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL3N+ms7iYkWgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a5f309e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple, rgb: (92, 23, 109)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3lKawd8D0mEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a5e8ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water, rgb: (160, 201, 202)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noDORxuLpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a5e666d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pink, rgb: (257, 153, 165)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a5e3ee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock, rgb: (83, 65, 52)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnVADcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27a5d9de80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_estimator = tf.estimator.Estimator(model_dir='pretrained', model_fn=model_fn)\n",
    "predict(pre_estimator, MY_TEST_INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Colorbot Solutions \n",
    "\n",
    "Here are the solutions to the exercises available at the colorbot notebook.\n",
    "\n",
    "In order to compare the models we encourage you to use Tensorboard and also use play_colorbot.py --model_dir=path_to_your_model to play with the models and check how it does with general words other than color words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *EXERCISE EXPERIMENT*\n",
    "\n",
    "When using experiments you should make sure you repeat the datasets the number of epochs desired since the experiment will \"run the for loop for you\". Also, you can add a parameter to run a number of steps instead, it will run until the dataset ends or the number of steps.\n",
    "\n",
    "You can add this cell to your colorbot notebook and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# small important detail, to train properly with the experiment you need to\n",
    "# repeat the dataset the number of epochs desired\n",
    "train_input_fn = get_input_fn(TRAIN_INPUT, BATCH_SIZE, num_epochs=40)\n",
    "\n",
    "# create experiment\n",
    "def generate_experiment_fn(run_config, hparams):\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=test_input_fn\n",
    "    )\n",
    "\n",
    "learn_runner.run(generate_experiment_fn, run_config=tf.contrib.learn.RunConfig(model_dir='model_dir'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *EXERCISE DATASET*\n",
    "\n",
    "0. Run the colorbot experiment and notice the choosen model_dir\n",
    "1. Below is the input function definition,we don't need some of the auxiliar functions anymore\n",
    "2. Add this cell and then add the solution to the EXERCISE EXPERIMENT\n",
    "3. choose a different model_dir and run the cells\n",
    "4. Copy the model_dir of the two models to the same path\n",
    "5. tensorboard --logdir=path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_fn(csv_file, batch_size, num_epochs=1, shuffle=True):\n",
    "    def _parse(line):\n",
    "        # each line: name, red, green, blue\n",
    "        # split line\n",
    "        items = tf.string_split([line],',').values\n",
    "\n",
    "        # get color (r, g, b)\n",
    "        color = tf.string_to_number(items[1:], out_type=tf.float32) / 255.0\n",
    "\n",
    "        # split color_name into a sequence of characters\n",
    "        color_name = tf.string_split([items[0]], '')\n",
    "        length = color_name.indices[-1, 1] + 1 # length = index of last char + 1\n",
    "        color_name = color_name.values\n",
    "        return color, color_name, length\n",
    "\n",
    "    def input_fn():\n",
    "        # https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data\n",
    "        dataset = (\n",
    "            tf.contrib.data.TextLineDataset(csv_file) # reading from the HD\n",
    "            .skip(1) # skip header\n",
    "            .map(_parse) # parse text to variables\n",
    "            .padded_batch(batch_size, padded_shapes=([None], [None], []),\n",
    "                               padding_values=(0.0, chr(0), tf.cast(0, tf.int64)))\n",
    "            \n",
    "            .repeat(num_epochs) # repeat dataset the number of epochs\n",
    "        )\n",
    "        \n",
    "        # for our \"manual\" test we don't want to shuffle the data\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=100000)\n",
    "\n",
    "        # create iterator\n",
    "        color, color_name, length = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        features = {\n",
    "            COLOR_NAME_KEY: color_name,\n",
    "            SEQUENCE_LENGTH_KEY: length,\n",
    "        }\n",
    "\n",
    "        return features, color\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result you will see something like:\n",
    "\n",
    "![](../../images/colorbot_dataset_exercise_sol.png)\n",
    "\n",
    "We called the original model \"sorted_batch\" and the model using the simplified input function as \"simple_batch\"\n",
    "\n",
    "Notice that both models have basically the same loss in the last step, but the \"sorted_batch\" model runs way faster , notice the `global_step/sec` metric, it measures how many steps the model executes per second. Since the \"sorted_batch\" has a larger `global_step/sec` it means it trains faster. \n",
    "\n",
    "If you don't belive me you can change Tensorboard to compare the models in a \"relative\" way, this will compare the models over time. See result below.\n",
    "\n",
    "![](../../images/colorbot_dataset_exercise_relative_sol.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *EXERCISE HYPERPARAMETERS*\n",
    "\n",
    "This one is more personal, what you see will depends on what you change in the model.\n",
    "Below is a very simple example we just changed the model to use a GRUCell, just in case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_fn(rnn_cell_sizes,\n",
    "                 label_dimension,\n",
    "                 dnn_layer_sizes=[],\n",
    "                 optimizer='SGD',\n",
    "                 learning_rate=0.01):\n",
    "    \n",
    "    def model_fn(features, labels, mode):\n",
    "        \n",
    "        color_name = features[COLOR_NAME_KEY]\n",
    "        sequence_length = tf.cast(features[SEQUENCE_LENGTH_KEY], dtype=tf.int32) # int64 -> int32\n",
    "        \n",
    "        # ----------- Preparing input --------------------\n",
    "        # Creating a tf constant to hold the map char -> index\n",
    "        # this is need to create the sparse tensor and after the one hot encode\n",
    "        mapping = tf.constant(CHARACTERS, name=\"mapping\")\n",
    "        table = tf.contrib.lookup.index_table_from_tensor(mapping, dtype=tf.string)\n",
    "        int_color_name = table.lookup(color_name)\n",
    "        \n",
    "        # representing colornames with one hot representation\n",
    "        color_name_onehot = tf.one_hot(int_color_name, depth=len(CHARACTERS) + 1)\n",
    "        \n",
    "        # ---------- RNN -------------------\n",
    "        # Each RNN layer will consist of a GRU cell\n",
    "        rnn_layers = [tf.nn.rnn_cell.GRUCell(size) for size in rnn_cell_sizes]\n",
    "        \n",
    "        # Construct the layers\n",
    "        multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "        \n",
    "        # Runs the RNN model dynamically\n",
    "        # more about it at: \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                                 inputs=color_name_onehot,\n",
    "                                                 sequence_length=sequence_length,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        # Slice to keep only the last cell of the RNN\n",
    "        last_activations = rnn_common.select_last_activations(outputs,\n",
    "                                                              sequence_length)\n",
    "\n",
    "        # ------------ Dense layers -------------------\n",
    "        # Construct dense layers on top of the last cell of the RNN\n",
    "        for units in dnn_layer_sizes:\n",
    "            last_activations = tf.layers.dense(\n",
    "              last_activations, units, activation=tf.nn.relu)\n",
    "        \n",
    "        # Final dense layer for prediction\n",
    "        predictions = tf.layers.dense(last_activations, label_dimension)\n",
    "\n",
    "        # ----------- Loss and Optimizer ----------------\n",
    "        loss = None\n",
    "        train_op = None\n",
    "\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:    \n",
    "            loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "    \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:    \n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss,\n",
    "              tf.contrib.framework.get_global_step(),\n",
    "              optimizer=optimizer,\n",
    "              learning_rate=learning_rate)\n",
    "        \n",
    "        return model_fn_lib.EstimatorSpec(mode,\n",
    "                                           predictions=predictions,\n",
    "                                           loss=loss,\n",
    "                                           train_op=train_op)\n",
    "    return model_fn"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "//experimental/users/jamieas/transform_colab:notebook",
    "kind": "private"
   },
   "name": "Copy of CustomEstimator.ipynb",
   "provenance": [
    {
     "file_id": "0BwN-JPfIIHwgdFkwUTVIWTQwU00",
     "timestamp": 1496845355496
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
